{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report of Project 3: Collaboration and Competition\n",
    "\n",
    "\n",
    "## Summary \n",
    "\n",
    "For this project,the Deep Deterministic Policy Gradient (DDPG) algorithm was used to train the agent.\n",
    "\n",
    "\n",
    "Because unity environment provide:\n",
    "- Number of agents: 2\n",
    "- Size of each action: 2\n",
    "- There are 2 agents. Each observes a state with length: 24\n",
    "So instance the 2 seprate agents with different networks train at the same time.\n",
    "I modified the maddpg code from udacity \"MADDPG - Lab\" in which train an agent to solve the Physical Deception problem.\n",
    "I simply the Critic input from (full observe + all action) to standard DDPG agent (single observe + single action), then output one Q value. But seprative train two agents.\n",
    "\n",
    "## Model architecture and hyperparameters\n",
    "\n",
    "The model architectures for the two neural networks used for the Actor and Critic are as follows:\n",
    "\n",
    "Actor:\n",
    "* Fully connected layer 1: Input 24 (state space), Output 256, and then RELU activation\n",
    "* Fully connected layer 2: Input 256, Output 128, RELU activation\n",
    "* Fully connected layer 3: Input 128, Output 2 (action space), TANH activation\n",
    "\n",
    "Critic:\n",
    "* Fully connected layer 1: Input 26 (state space), Output 256, Batch Normalization, RELU activation\n",
    "* Fully connected layer 2: Input 256, Output 128, RELU activation\n",
    "* Fully connected layer 3: Input 128, Output 1\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "```\n",
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "batchsize = 1000        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-3         # learning rate of the actor \n",
    "LR_CRITIC = 1e-4        # learning rate of the critic\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Plot of rewards\n",
    "\n",
    "Below is a training run of the above model archicture and hyperparameters,I load back the episode rewards file and plot the rewards.\n",
    "\n",
    "* Environment solved in about 1906 episodes,\tAverage Score: 0.51; in 2036 episode, it reaches 1.51 average score.\n",
    "![print](./print.png)\n",
    "![rewards](./ddpg.png)\n",
    "\n",
    "\n",
    "\n",
    "###  Test  \n",
    "\n",
    "\n",
    "Loading back the parameters checkpoint, and test the trained agent performances.\n",
    "![test](./test_agent.png)\n",
    "\n",
    "Please watch the [video of trained agent testing](https://youtu.be/0k_-0Om0o6A)\n",
    "\n",
    "\n",
    "\n",
    "## Future work\n",
    "\n",
    "* Change hyper-parameters,to accerate the training speed.\n",
    "* Apply MaDDPG algorithm uses a ‘decentralised actor, centralised critic training approach’\n",
    "\n",
    "# Troubleshooting Tips\n",
    "If using the CUDA, please make sure all CUDA version is compatible with other packages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
